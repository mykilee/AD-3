{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z81sDP-WhbM4"
   },
   "source": [
    "# Assignment 3: LDA Topic Modeling\n",
    "\n",
    "## Note\n",
    "Installing Tomotopy locally can return an error, if that's the case run this notebook on Google Colab\n",
    "\n",
    "## Research Background\n",
    "\n",
    "LDA is a popular topic modeling algorithm widely used in the fields of Digital Humanities and Social Sciences. In the field of political communication, topic modeling is often applied for analyzing politicians Twitter/X posts, identitying thematic patterns or topics revolving around their posts.\n",
    "\n",
    "For this assignment, students will work with tweets from two USA politicians, Donald Trump and Bernie Sanders, who are often regarded as right-wing populist and left-wing populist respectively. Right-wing populism often emphasizes nationalism, anti-immigration policies, and a critique of global elites from a culturally conservative perspective, focusing on preserving traditional values and social hierarchies. Left-wing populism prioritizes economic inequality, advocating for the redistribution of wealth, expansion of social services, and empowerment of the working class against the capitalist elite. While both forms of populism appeal to the \"common people\" against perceived elites and established structures, they diverge significantly in their identification of the elites, proposed solutions, and core ideologies. For a more detailed explanation, you can read the chapter by Macaulay (2019) \"Bernie and The Donald: A comparison of left-and right-wing populist discourse\" (full reference below).\n",
    "\n",
    "**Research Questions**\n",
    "1. What topics are revolving around Donald Trump and Bernie Sanders' posts separately?\n",
    "2. What are the topic differences between Trump (right-wing popoulist) and Sanders (left-wing populist)?\n",
    "\n",
    "**Aim:**\n",
    "1. The first aim of the assignment is to conduct LDA topic modeling. Identify thematic patterns or politics revolving around Trump or Sanders's posts.\n",
    "2. The second aim is to critically evaluate the results of topic modeling. Try different numbers of topics to see with which settings the topics are more coherent. Critically reflect on the results of LDA topic modeling, discussing them in relation to existing theories about populism.\n",
    "\n",
    "**Data**\n",
    "Two datasets are prepared for this assginment. Tweets from Trump and tweets from Sanders. Students are asked to work on these two datasets.\n",
    "\n",
    "**Methods**\n",
    "1. Word segamentation\n",
    "2. Removing stopwords\n",
    "3. LDA topic modeling\n",
    "4. Topic evaulation (coherence and human evaluation)\n",
    "5. Visualization of results.\n",
    "\n",
    "**References**\n",
    "1. Macaulay, M. (2019). Bernie and the Donald: A comparison of Left-and Right-wing populist discourse. *Populist discourse: International perspectives*, 165-195.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOAzU-HhhbM9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4wYU9mJM99W"
   },
   "source": [
    "### Q1. Install necessary libraries, including `tomotopy` and `little_mallet_wrapper`, and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SFH5FrRzhbM-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tomotopy in /Users/mykilee/anaconda3/lib/python3.11/site-packages (0.12.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/mykilee/anaconda3/lib/python3.11/site-packages (from tomotopy) (1.24.3)\n",
      "Requirement already satisfied: little_mallet_wrapper in /Users/mykilee/anaconda3/lib/python3.11/site-packages (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Q1 (code)\n",
    "!pip install tomotopy\n",
    "!pip install little_mallet_wrapper\n",
    "# Import libraries\n",
    "import tomotopy as tp\n",
    "import little_mallet_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4LzJUC3hbNC"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1qdFBgYM3_Z"
   },
   "source": [
    "### Q2. Load the two datasets and concatenate them\n",
    "\n",
    "The goal is to run topic modelling on the combined dataset of Sanders and Trump's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gc1Wj2K3hbNE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SenSanders</td>\n",
       "      <td>1351670845639421952</td>\n",
       "      <td>SenSanders_1351670845639421952</td>\n",
       "      <td>Wealth increase in past 10 months:\\n⬆️$173 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SenSanders</td>\n",
       "      <td>1351259762722279424</td>\n",
       "      <td>SenSanders_1351259762722279424</td>\n",
       "      <td>Amidst massive income and wealth inequality te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SenSanders</td>\n",
       "      <td>1351242822293319680</td>\n",
       "      <td>SenSanders_1351242822293319680</td>\n",
       "      <td>“We now have the resources, we now have the sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SenSanders</td>\n",
       "      <td>1350885541911400448</td>\n",
       "      <td>SenSanders_1350885541911400448</td>\n",
       "      <td>After surviving an attempt on his life, Russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SenSanders</td>\n",
       "      <td>1349880150041780224</td>\n",
       "      <td>SenSanders_1349880150041780224</td>\n",
       "      <td>President-Elect Biden's COVID rescue plan will...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Username              TweetId                           Title  \\\n",
       "0  SenSanders  1351670845639421952  SenSanders_1351670845639421952   \n",
       "1  SenSanders  1351259762722279424  SenSanders_1351259762722279424   \n",
       "2  SenSanders  1351242822293319680  SenSanders_1351242822293319680   \n",
       "3  SenSanders  1350885541911400448  SenSanders_1350885541911400448   \n",
       "4  SenSanders  1349880150041780224  SenSanders_1349880150041780224   \n",
       "\n",
       "                                             Content  \n",
       "0  Wealth increase in past 10 months:\\n⬆️$173 bil...  \n",
       "1  Amidst massive income and wealth inequality te...  \n",
       "2  “We now have the resources, we now have the sk...  \n",
       "3  After surviving an attempt on his life, Russia...  \n",
       "4  President-Elect Biden's COVID rescue plan will...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q2 (code)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets for Sanders and Trump's tweets\n",
    "sanders_tweets = pd.read_csv(\"sanders_tweets.csv\")\n",
    "trump_tweets = pd.read_csv(\"trump_tweets.csv\")\n",
    "\n",
    "# Concatenate the datasets into a single dataframe\n",
    "combined_tweets = pd.concat([sanders_tweets, trump_tweets])\n",
    "\n",
    "# Display the combined dataframe as a table\n",
    "display(combined_tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr84dAOvhbNE"
   },
   "source": [
    "### Q3. Clean the data\n",
    "\n",
    "Transform all tweets to lowercase, remove stopwords, punctuation, and numbers. Add the processed text to a list called `training_data`.\n",
    "Create a list with the content of the tweets (`original_texts`) and a list that allows you to identify both the author of the tweet and its ID (`titles`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mykilee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mykilee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'wealth increase past months billion elon musk billion jeff bezos billion walton family billion bill gates billion mark zuckerberg increase minimum wage must tax rich amp raise minimum wage least'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 (code)\n",
    "# Tip: add the following line to remove URLS and user mentions\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lists\n",
    "training_data = []\n",
    "original_texts = []\n",
    "titles = []\n",
    "\n",
    "# Define stopwords and compile regex for punctuation and numbers\n",
    "stop_words = set(stopwords.words('english')) # Initialize a set of English stopwords\n",
    "\n",
    "# Iterate through each row of the combined dataset\n",
    "for index, row in combined_tweets.iterrows():\n",
    "    original_text = row['Content'].lower() # Extract original text and convert it to lowercase\n",
    "    original_texts.append(original_text) # Append original text to the list\n",
    "    \n",
    "    # Remove URLs, user mentions, and specific keywords using regular expressions\n",
    "    processed_text = re.sub(r\"http\\S+|www\\S+|https\\S+|\\/\\/t|co\\/|\\@\\w+|realdonaldtrump|rt\", '', original_text, flags=re.MULTILINE)\n",
    "    words = word_tokenize(processed_text) # Tokenize the processed text into words\n",
    "    \n",
    "    # Filter out stopwords and non-alphabetic characters using list comprehension\n",
    "    words_filtered = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "\n",
    "    training_data.append(' '.join(words_filtered))\n",
    "    # Combine the username and tweet ID and append it as a title to the titles list\n",
    "    titles.append(f\"{row['Username']} - {row['TweetId']}\")\n",
    "\n",
    "training_data[0]# Output the first processed text for checking the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeKlV0IehbNH"
   },
   "source": [
    "## LDA topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhTQrfCkHbXS"
   },
   "source": [
    "### Q4. Train a an LDA topic model with `tomotopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "lKrCy-P6hbNI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Model Training...\n",
      "\n",
      "\n",
      "Iteration: 0\tLog-likelihood: -9.215070317680787\n",
      "Iteration: 10\tLog-likelihood: -8.816612327333274\n",
      "Iteration: 20\tLog-likelihood: -8.656484676123368\n",
      "Iteration: 30\tLog-likelihood: -8.5716307448915\n",
      "Iteration: 40\tLog-likelihood: -8.510036432036635\n",
      "Iteration: 50\tLog-likelihood: -8.462078186829734\n",
      "Iteration: 60\tLog-likelihood: -8.432362482210593\n",
      "Iteration: 70\tLog-likelihood: -8.404993381079537\n",
      "Iteration: 80\tLog-likelihood: -8.384460858293908\n",
      "Iteration: 90\tLog-likelihood: -8.370021417414748\n"
     ]
    }
   ],
   "source": [
    "# Q4 (code)\n",
    "# Number of topics to return\n",
    "num_topics = 15\n",
    "# Numer of topic words to print out\n",
    "num_topic_words = 10\n",
    "\n",
    "# Intialize the model\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Add each document to the model, after splitting it up into words\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "\n",
    "print(\"Topic Model Training...\\n\\n\")\n",
    "# Iterate over the data 10 times\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "    print(f'Iteration: {i}\\tLog-likelihood: {model.ll_per_word}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucG5gYghbNI"
   },
   "source": [
    "### Q5. Print out the top words for each topic and manually evaluate their coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "fu_F1j17hbNJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Model Results:\n",
      "\n",
      "\n",
      "✨Topic 0✨\n",
      "\n",
      "jobs economy workers years wage american ever since year record\n",
      "\n",
      "✨Topic 1✨\n",
      "\n",
      "democrats impeachment president election house schiff trump senate nothing republican\n",
      "\n",
      "✨Topic 2✨\n",
      "\n",
      "democrats border amp want wall country security get would people\n",
      "\n",
      "✨Topic 3✨\n",
      "\n",
      "states united china trade deal president amp countries many world\n",
      "\n",
      "✨Topic 4✨\n",
      "\n",
      "great big thank america noh make see forward house korea\n",
      "\n",
      "✨Topic 5✨\n",
      "\n",
      "news fake cou joe new like biden amp trump supreme\n",
      "\n",
      "✨Topic 6✨\n",
      "\n",
      "must trump federal war act congress senate people government suppo\n",
      "\n",
      "✨Topic 7✨\n",
      "\n",
      "fbi russia trump collusion witch mueller hunt amp campaign hillary\n",
      "\n",
      "✨Topic 8✨\n",
      "\n",
      "news fake media amp people great much would never even\n",
      "\n",
      "✨Topic 9✨\n",
      "\n",
      "health care people tax americans need must pay million make\n",
      "\n",
      "✨Topic 10✨\n",
      "\n",
      "great thank today honor day women president amp american america\n",
      "\n",
      "✨Topic 11✨\n",
      "\n",
      "trump president live tonight watch join donald maga enjoy national\n",
      "\n",
      "✨Topic 12✨\n",
      "\n",
      "change climate energy fossil biden planet fuel joe trump new\n",
      "\n",
      "✨Topic 13✨\n",
      "\n",
      "great total endorsement vote state amp military job strong complete\n",
      "\n",
      "✨Topic 14✨\n",
      "\n",
      "people country must american us stand fight trump america back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q5a (code)\n",
    "print(\"\\nTopic Model Results:\\n\\n\")\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSeonT734KsL"
   },
   "source": [
    "# Q5b (words)\n",
    "# Describe what each topic is about. What ideas, values, or situations do these keywords refer to?\n",
    "\n",
    "\n",
    "Topic 0: This topic appears to focus on economic issues, such as jobs, wages, and workers, suggesting discussions related to employment, income, and labor conditions.\n",
    "\n",
    "Topic 1: This topic revolves around political matters, particularly concerning Democrats, impeachment, elections, and the political dynamics between the presidency, Congress, and the Senate.\n",
    "\n",
    "Topic 2: This topic is centered on border security and immigration, with discussions related to Democrats' stance on building a wall, national security concerns, and immigration policies.\n",
    "\n",
    "Topic 3: This topic relates to international relations and trade, mentioning the United States, China, trade deals, and global economic interactions.\n",
    "\n",
    "Topic 4: This topic expresses gratitude and positivity, focusing on appreciating great achievements, America, and the progress made in various endeavors.\n",
    "\n",
    "Topic 5: This topic discusses media and political discourse, addressing issues such as fake news, Joe Biden, and skepticism towards the media and political narratives.\n",
    "\n",
    "Topic 6: This topic concerns governmental actions and policies, including federal initiatives, wars, and the role of Congress in supporting governmental endeavors.\n",
    "\n",
    "Topic 7: This topic delves into investigations and political controversies, mentioning the FBI, Russia, collusion, and the Mueller investigation, particularly in the context of the 2016 presidential campaign.\n",
    "\n",
    "Topic 8: This topic examines media and public perception, highlighting discussions surrounding fake news, media credibility, and skepticism towards mainstream narratives.\n",
    "\n",
    "Topic 9: This topic focuses on social welfare and economic concerns, addressing issues such as healthcare, taxes, and financial needs affecting Americans.\n",
    "\n",
    "Topic 10: This topic celebrates and honors achievements and contributions, particularly those of women and Americans, while expressing gratitude for their efforts and contributions.\n",
    "\n",
    "Topic 11: This topic discusses events and support for the current presidency, including live events, watching, and supporting President Trump.\n",
    "\n",
    "Topic 12: This topic addresses environmental issues, including climate change, energy sources, and the transition to renewable energy, with mentions of fossil fuels and Biden's stance.\n",
    "\n",
    "Topic 13: This topic highlights endorsements and support, particularly in the context of voting, military service, and job opportunities.\n",
    "\n",
    "Topic 14: This topic emphasizes unity and patriotism, urging people to stand up and fight for the country and American values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_Cv23XVAj8E"
   },
   "source": [
    "## Topic coherence\n",
    "\n",
    "Use `tomotopy`'s [`.coherence()`](https://bab2min.github.io/tomotopy/v0.10.0/en/coherence.html) function to automatically calculate the topic coherence.\n",
    "\n",
    "The coherence value can vary from `0` (no coherence) to `1` (maximum coherence). Interpret the results and, if needed, retrain the model using a different number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "gzDijy831LHb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Coherence : c_v ====\n",
      "Average: 0.5482050222655137 \n",
      "Per Topic: [0.5498744729906321, 0.5977268993854523, 0.5335139751434326, 0.43302450627088546, 0.5083450015634299, 0.49551205039024354, 0.5735689289867878, 0.46056297421455383, 0.6199014991521835, 0.48265000283718107, 0.5368260830640793, 0.6344104558229446, 0.5752459466457367, 0.47429473847150805, 0.7476177990436554]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are different metrics for coherence, we choose `c_v`\n",
    "\n",
    "coh = tp.coherence.Coherence(model, coherence='c_v')\n",
    "average_coherence = coh.get_score()\n",
    "coherence_per_topic = [coh.get_score(topic_id=k) for k in range(model.k)]\n",
    "\n",
    "print('==== Coherence : {} ===='.format('c_v'))\n",
    "print('Average:', average_coherence, '\\nPer Topic:', coherence_per_topic)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Model Training...\n",
      "\n",
      "\n",
      "Iteration: 0\tLog-likelihood: -9.245070281451664\n",
      "Iteration: 10\tLog-likelihood: -8.85359431815452\n",
      "Iteration: 20\tLog-likelihood: -8.677549417246743\n",
      "Iteration: 30\tLog-likelihood: -8.584507159265351\n",
      "Iteration: 40\tLog-likelihood: -8.523444494403455\n",
      "Iteration: 50\tLog-likelihood: -8.47921080364053\n",
      "Iteration: 60\tLog-likelihood: -8.442400145306161\n",
      "Iteration: 70\tLog-likelihood: -8.415463069503907\n",
      "Iteration: 80\tLog-likelihood: -8.392873905214461\n",
      "Iteration: 90\tLog-likelihood: -8.374455883256594\n",
      "==== Coherence : c_v ====\n",
      "Average: 0.5357297073056301 \n",
      "Per Topic: [0.4824100710451603, 0.6527347683906555, 0.5498155765235424, 0.6012917518615722, 0.7441084325313568, 0.5671304583549499, 0.5726846009492874, 0.46798570938408374, 0.462851881980896, 0.4119297653436661, 0.42636522427201273, 0.5326727986335754, 0.49516292214393615, 0.5947236262261868, 0.4740780219435692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrain \n",
    "\n",
    "# Number of topics to return\n",
    "num_topics = 15\n",
    "# Numer of topic words to print out\n",
    "num_topic_words = 10\n",
    "\n",
    "# Intialize the model\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Add each document to the model, after splitting it up into words\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "\n",
    "print(\"Topic Model Training...\\n\\n\")\n",
    "# Iterate over the data 10 times\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "    print(f'Iteration: {i}\\tLog-likelihood: {model.ll_per_word}')\n",
    "coh = tp.coherence.Coherence(model, coherence='c_v')\n",
    "average_coherence = coh.get_score()\n",
    "coherence_per_topic = [coh.get_score(topic_id=k) for k in range(model.k)]\n",
    "\n",
    "print('==== Coherence : {} ===='.format('c_v'))\n",
    "print('Average:', average_coherence, '\\nPer Topic:', coherence_per_topic)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrEHnkB-Cs4_"
   },
   "source": [
    "### Q6. Interpret topic coherence\n",
    "\n",
    "Report the following:\n",
    "- number of topics you initially used to train the model and the coherence score you got\n",
    "- changes made to the number of topics and new coherence scores obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnXNA_m2DD0A"
   },
   "source": [
    "# Q6 (words)\n",
    "\n",
    "\n",
    "The initial model was trained using 15 topics, with an average coherence score of approximately 0.548 (c_v metric). The coherence scores for individual topics ranged from 0.41 to 0.75, exhibiting a wide distribution, with some topics showing relatively higher coherence scores. \n",
    "\n",
    "Upon retraining the model with the same number of topics (15), the new coherence score averaged around 0.536 (c_v metric). After retraining, the average coherence score slightly decreased to 0.536, but overall remained comparable to the initial model. While some topics experienced improvements in coherence scores, others exhibited decreases. \n",
    "\n",
    "In summary, although the retrained model showed a slight decrease in average coherence, it remained relatively consistent with the initial model. Further improvements in coherence may require adjustments to other parameters or experimentation with different modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpZbWHxfBr1E"
   },
   "source": [
    "### X1. Optional question 1\n",
    "(This question is not compulsory, it only allows you to get an extra point.)\n",
    "\n",
    "Create a function to plot the average coherence for models with different number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 31\u001b[0m plot_coherence(min_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, training_data\u001b[38;5;241m=\u001b[39mtraining_data, num_topic_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mplot_coherence\u001b[0;34m(min_topics, max_topics, step, training_data, num_topic_words)\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mLDAModel(k\u001b[38;5;241m=\u001b[39mnum_topics)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m training_data:\n\u001b[0;32m---> 13\u001b[0m     model\u001b[38;5;241m.\u001b[39madd_doc(text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Train the model for 100 iterations\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate coherence score\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# X1 (code)\n",
    "# Tip: y = average topic coherence; x = number of topics in the model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coherence(min_topics, max_topics, step, training_data, num_topic_words):\n",
    "    coherence_scores = []\n",
    "    num_topics_list = []\n",
    "    \n",
    "    for num_topics in range(min_topics, max_topics+1, step):\n",
    "        # Initialize and train the LDA model\n",
    "        model = tp.LDAModel(k=num_topics)\n",
    "        for text in training_data:\n",
    "            model.add_doc(text.split())\n",
    "        model.train(iter=100)  # Train the model for 100 iterations\n",
    "        \n",
    "        # Calculate coherence score\n",
    "        coh = tp.coherence.Coherence(model, coherence='c_v')\n",
    "        average_coherence = coh.get_score()\n",
    "        coherence_scores.append(average_coherence)\n",
    "        num_topics_list.append(num_topics)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(num_topics_list, coherence_scores, marker='o')\n",
    "    plt.title('Average Coherence vs. Number of Topics')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Average Coherence')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_coherence(min_topics=5, max_topics=30, step=5, training_data=training_data, num_topic_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_coherence_scores() got an unexpected keyword argument 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 32\u001b[0m coherence_scores \u001b[38;5;241m=\u001b[39m calculate_coherence_scores(training_data, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     33\u001b[0m plot_coherence_scores(coherence_scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_coherence_scores() got an unexpected keyword argument 'start'"
     ]
    }
   ],
   "source": [
    "# X1 (code)\n",
    "# Tip: y = average topic coherence; x = number of topics in the modelimport matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coherence(min_topics, max_topics, step, training_data, iterations):\n",
    "    coherence_scores = []\n",
    "\n",
    "    for num_topics in range(min_topics, max_topics + 1, step):\n",
    "        # Initialize the LDA model\n",
    "        model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "        # Add documents to the model\n",
    "        for text in training_data:\n",
    "            model.add_doc(text.strip().split())\n",
    "\n",
    "        # Train the model\n",
    "        for i in range(0, iterations * 10, iterations):\n",
    "            model.train(iterations)\n",
    "\n",
    "        # Calculate coherence score\n",
    "        coh = tp.coherence.Coherence(model, coherence='c_v')\n",
    "        average_coherence = coh.get_score()\n",
    "        coherence_scores.append(average_coherence)\n",
    "\n",
    "    # Plot the coherence scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(min_topics, max_topics + 1, step), coherence_scores, marker='o')\n",
    "    plt.title('Average Coherence vs. Number of Topics')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Average Coherence')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "coherence_scores = calculate_coherence_scores(training_data, start=2, end=20, step=2)\n",
    "plot_coherence_scores(coherence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYY_TIOyDLnN"
   },
   "source": [
    "### Q7. Topic distributions\n",
    "Calculate the topic distributions for all tweets and get the top documents for some topics (between 2 and 5) that you think could be more representative of Sanders or Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top documents for Topic 1:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_md' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_id \u001b[38;5;129;01min\u001b[39;00m topics_of_interest:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop documents for Topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     get_top_docs(original_texts, topic_distributions, topics, topic_index\u001b[38;5;241m=\u001b[39mtopic_id, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mget_top_docs\u001b[0;34m(docs, topic_distributions, topics, topic_index, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m sorted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([(_distribution[topic_index], _document)\n\u001b[1;32m      8\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m _distribution, _document\n\u001b[1;32m      9\u001b[0m                       \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(topic_distributions, docs)], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m topic_words \u001b[38;5;241m=\u001b[39m topics[topic_index]\n\u001b[0;32m---> 13\u001b[0m make_md(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### ✨Topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m✨\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m probability, doc \u001b[38;5;129;01min\u001b[39;00m sorted_data[:n]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Make topic words bolded\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_md' is not defined"
     ]
    }
   ],
   "source": [
    "def get_top_docs(docs, topic_distributions, topics, topic_index, n=5):\n",
    "    # Check if the selected topic index is valid\n",
    "    if topic_index >= len(topic_distributions):\n",
    "        print(f\"Error: Topic index {topic_index} is out of range. Number of topics in the model: {len(topic_distributions)}\")\n",
    "        return\n",
    "    \n",
    "    sorted_data = sorted([(_distribution[topic_index], _document)\n",
    "                          for _distribution, _document\n",
    "                          in zip(topic_distributions, docs)], reverse=True)\n",
    "    \n",
    "    topic_words = topics[topic_index]\n",
    "    \n",
    "    make_md(f\"### ✨Topic {topic_index}✨\\n\\n{topic_words}\\n\\n\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    for probability, doc in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_words.split():\n",
    "            if word in doc.lower():\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        \n",
    "        make_md(f'✨  \\n**Topic Probability**: {probability}  \\n**Document**: {doc}\\n\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "# Display 3 documents for topic 1，2，3，4\n",
    "\n",
    "topics_of_interest = [1, 2, 3, 4]  \n",
    "\n",
    "# Loop through the topics of interest and display the top documents for each topic\n",
    "for topic_id in topics_of_interest:\n",
    "    print(f\"Top documents for Topic {topic_id}:\")\n",
    "    get_top_docs(original_texts, topic_distributions, topics, topic_index=topic_id, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top documents for Topic 1:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_md' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_id \u001b[38;5;129;01min\u001b[39;00m topics_of_interest:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop documents for Topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     get_top_docs(original_texts, topic_distributions, topics, topic_index\u001b[38;5;241m=\u001b[39mtopic_id, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mget_top_docs\u001b[0;34m(docs, topic_distributions, topics, topic_index, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m sorted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([(_distribution[topic_index], _document)\n\u001b[1;32m      8\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m _distribution, _document\n\u001b[1;32m      9\u001b[0m                       \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(topic_distributions, docs)], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m topic_words \u001b[38;5;241m=\u001b[39m topics[topic_index]\n\u001b[0;32m---> 13\u001b[0m make_md(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### ✨Topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m✨\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m probability, doc \u001b[38;5;129;01min\u001b[39;00m sorted_data[:n]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Make topic words bolded\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_md' is not defined"
     ]
    }
   ],
   "source": [
    "def get_top_docs(docs, topic_distributions, topics, topic_index, n=5):\n",
    "    # Check if the selected topic index is valid\n",
    "    if topic_index >= len(topic_distributions):\n",
    "        print(f\"Error: Topic index {topic_index} is out of range. Number of topics in the model: {len(topic_distributions)}\")\n",
    "        return\n",
    "    \n",
    "    sorted_data = sorted([(_distribution[topic_index], _document)\n",
    "                          for _distribution, _document\n",
    "                          in zip(topic_distributions, docs)], reverse=True)\n",
    "    \n",
    "    topic_words = topics[topic_index]\n",
    "    \n",
    "    make_md(f\"### ✨Topic {topic_index}✨\\n\\n{topic_words}\\n\\n\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    for probability, doc in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_words.split():\n",
    "            if word in doc.lower():\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        \n",
    "        make_md(f'✨  \\n**Topic Probability**: {probability}  \\n**Document**: {doc}\\n\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "# Display 3 documents for topic 1，2，3，4\n",
    "\n",
    "topics_of_interest = [1, 2, 3, 4]  \n",
    "\n",
    "# Loop through the topics of interest and display the top documents for each topic\n",
    "for topic_id in topics_of_interest:\n",
    "    print(f\"Top documents for Topic {topic_id}:\")\n",
    "    get_top_docs(original_texts, topic_distributions, topics, topic_index=topic_id, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nyQ7GnIDrWr"
   },
   "source": [
    "Interpret the results above. Are there topics that have top tweets only by one politician? Why do you think these topics are more representative of one of the two politicians' views?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNXy1koX6sFN"
   },
   "source": [
    "# Q7b (words)\n",
    "Topic 1: The top tweets in this topic primarily discuss healthcare, criticism of Republican policies, and immigration issues, which are topics often emphasized by Bernie Sanders. These tweets reflect Sanders' perspective on healthcare accessibility, opposition to Republican proposals, and concerns about the immigration system, making this topic more representative of Sanders' views.\n",
    "\n",
    "Topic 2: The top tweets in this topic focus on issues related to border security, corruption allegations against Democrats, and natural disasters. While these topics may be of interest to both politicians, the emphasis on border security and criticism of Democrats align more closely with Trump's viewpoints, making this topic potentially more representative of Trump's views.\n",
    "\n",
    "Topic 3: The top tweets in this topic discuss topics such as patriotism, remembrance of historical events, and gratitude towards military personnel. These themes resonate with Trump's rhetoric emphasizing national pride, support for the military, and commemoration of significant events in American history, suggesting that this topic is more representative of Trump's views.\n",
    "\n",
    "Topic 4: The top tweets in this topic mention various political figures, express sentiments of gratitude, and comment on current events like the COVID-19 pandemic. This topic covers a broad range of subjects and does not strongly align with the specific views of either politician, making it less representative of either Sanders or Trump.\n",
    "\n",
    "Overall, while some topics have top tweets that are more aligned with one politician's views, there are also topics that encompass a broader range of issues and are less indicative of a specific politician's perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LxDb31D01p"
   },
   "source": [
    "## Large scale analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXmHmjaHI2fD"
   },
   "source": [
    "### Q8. Create a random sample of the whole dataset and visualize the topic distributions for the sampled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "SQdtQCJQiBUw"
   },
   "outputs": [],
   "source": [
    "# Crete a sample of tweets\n",
    "\n",
    "from random import sample\n",
    "\n",
    "target_labels = sample(titles,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Af2ejUqBhtm9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     41\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     43\u001b[0m plot_categories_by_topics_heatmap(titles,\n\u001b[1;32m     44\u001b[0m                                   topic_distributions,\n\u001b[1;32m     45\u001b[0m                                   topic_individual_words,\n\u001b[0;32m---> 46\u001b[0m                                   target_labels\u001b[38;5;241m=\u001b[39mtarget_labels,\n\u001b[1;32m     47\u001b[0m                                   color_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     48\u001b[0m                                  dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Q8 (code)\n",
    "# Create a heatmap using the random sample\n",
    "# Tip: to display more than 20 tweets you have to change the values of `dim =` in sns.heatmap()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "def plot_categories_by_topics_heatmap(labels,\n",
    "                                      topic_distributions,\n",
    "                                      topic_keys,\n",
    "                                      output_path=None,\n",
    "                                      target_labels=None,\n",
    "                                      color_map = sns.cm.rocket_r,\n",
    "                                      dim=25):\n",
    "\n",
    "    # Combine the labels and distributions into a list of dictionaries.\n",
    "    dicts_to_plot = []\n",
    "    for _label, _distribution in zip(labels, topic_distributions):\n",
    "        if not target_labels or _label in target_labels:\n",
    "            for _topic_index, _probability in enumerate(_distribution):\n",
    "                dicts_to_plot.append({'Probability': float(_probability),\n",
    "                                      'Category': _label,\n",
    "                                      'Topic': 'Topic ' + str(_topic_index).zfill(2) + ': ' + ' '.join(topic_keys[_topic_index][:5])})\n",
    "\n",
    "    # Create a dataframe, format it for the heatmap function, and normalize the columns.\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(index='Category',\n",
    "                                     columns='Topic',\n",
    "                                     values='Probability')\n",
    "    df_norm_col=(df_wide-df_wide.mean())/df_wide.std()\n",
    "\n",
    "    # Show the final plot.\n",
    "    if dim:\n",
    "        plt.figure(figsize=dim)\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    ax = sns.heatmap(df_norm_col, cmap=color_map)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.xticks(rotation=30, ha='left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_categories_by_topics_heatmap(titles,\n",
    "                                  topic_distributions,\n",
    "                                  topic_individual_words,\n",
    "                                  target_labels=target_labels,\n",
    "                                  color_map = 'Blues',\n",
    "                                 dim=(25,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trwbJnmGE2VR"
   },
   "source": [
    "### Q9. Interpret the heatmap\n",
    "Do you see any pattern in the probability distributions of topics for each politician?\n",
    "\n",
    "Are there topics that are more likely for one of the two politicians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NPJXZUIIpor"
   },
   "source": [
    "# Q9 (words)\n",
    "The colors in the heatmap range from light (low probability) to dark (high probability), corresponding to how strongly a particular topic is represented in each tweet. If certain topics have consistently higher probabilities for one politician compared to the other, that would suggest those topics are more characteristic or prioritized by that politician in their public communication.\n",
    "\n",
    "For Trump:\n",
    "\n",
    "Topic 1: This topic includes keywords related to impeachment, Democrats, and elections, which align with Trump's frequent criticisms of Democrats and the impeachment proceedings against him.\n",
    "Topic 2: Keywords like \"democrats,\" \"border,\" and \"wall\" suggest a focus on immigration and border security, which are key issues in Trump's platform.\n",
    "Topic 4: Words like \"great,\" \"thank,\" and \"America\" are consistent with Trump's messaging emphasizing national pride and achievements.\n",
    "\n",
    "\n",
    "For Sanders:\n",
    "\n",
    "Topic 0: Terms such as \"jobs,\" \"economy,\" and \"workers\" indicate a focus on economic issues and workers' rights, which are central to Sanders' platform.\n",
    "Topic 9: Keywords like \"health care,\" \"tax,\" and \"Americans\" suggest discussions around healthcare and taxation, key topics in Sanders' policy proposals.\n",
    "Topic 14: Words like \"people,\" \"country,\" and \"American\" reflect themes of populism and national unity, which are often emphasized in Sanders' rhetoric.\n",
    "\n",
    "\n",
    "These associations are based on the keywords provided for each topic and the typical themes and priorities associated with each politician's messaging and policy positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYZ9qZ2CEACM"
   },
   "source": [
    "### X2. Optional question 2\n",
    "(This question is not compulsory, it only allows you to get an extra point)\n",
    "\n",
    "Make the sample balanced, with 50 tweets by Trump and 50 by Sanders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced sample shape: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "# Sample 50 tweets from each author\n",
    "sample_trump = trump_tweets.sample(n=50, random_state=42)\n",
    "sample_sanders = sanders_tweets.sample(n=50, random_state=42)\n",
    "\n",
    "# Concatenate the balanced samples\n",
    "balanced_sample = pd.concat([sample_trump, sample_sanders], ignore_index=True)\n",
    "\n",
    "# Check the shape of the balanced sample\n",
    "print(\"Balanced sample shape:\", balanced_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVvkS8XaFCmL"
   },
   "source": [
    "### X3. Optional question 3\n",
    "(This question is not compulsory, it only allows you to get an extra point)\n",
    "\n",
    "Extend the analysis to all the tweets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJFY02h08kIh"
   },
   "outputs": [],
   "source": [
    "# X3 (code and words)\n",
    "# Tip: plotting a heatmap for thousands of tweets is not practical.\n",
    "# Make a comparison based on the numerical values in the `df_norm_col` dataframe (see Week 6 notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_categories_by_topics_heatmap(labels,\n",
    "                                      topic_distributions,\n",
    "                                      topic_keys,\n",
    "                                      output_path=None,\n",
    "                                      target_labels=None,\n",
    "                                      color_map='Blues',\n",
    "                                      dim=None):\n",
    "\n",
    "    # Combine the labels and distributions into a list of dictionaries.\n",
    "    dicts_to_plot = []\n",
    "    for _label, _distribution in zip(labels, topic_distributions):\n",
    "        if not target_labels or _label in target_labels:\n",
    "            for _topic_index, _probability in enumerate(_distribution):\n",
    "                if _topic_index < len(topic_keys):  # Check if _topic_index is within range of topic_keys\n",
    "                    dicts_to_plot.append({'Probability': float(_probability),\n",
    "                                          'Category': _label,\n",
    "                                          'Topic': 'Topic ' + str(_topic_index).zfill(2) + ': ' + ' '.join(topic_keys[_topic_index][:5])})\n",
    "\n",
    "    # Create a dataframe from the dictionaries.\n",
    "    import pandas as pd\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "\n",
    "    # Pivot the dataframe to wide format and normalize the columns.\n",
    "    df_wide = df_to_plot.pivot_table(index='Category',\n",
    "                                     columns='Topic',\n",
    "                                     values='Probability')\n",
    "    df_norm_col = (df_wide - df_wide.mean()) / df_wide.std()\n",
    "    # Calculate summary statistics\n",
    "    summary_stats = df_norm_col.describe()\n",
    "    print(summary_stats)\n",
    "\n",
    "    # Visualize the data using box plots\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df_norm_col)\n",
    "    plt.title('Box Plot of Normalized Topic Distributions')\n",
    "    plt.xlabel('Topics')\n",
    "    plt.ylabel('Normalized Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
